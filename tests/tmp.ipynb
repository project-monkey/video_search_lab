{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab12522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., 11., 22., 33., 44., 55., 66., 77., 88., 99.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ans = np.linspace(start = 0, stop = 99, num=10)\n",
    "ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea808fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "提取视频均值特征 (CLIP ViT-B/32)\n",
    "python src/extract_clip.py --video_dir data/samples --out feats.npy\n",
    "\"\"\"\n",
    "import argparse, glob, os, json, torch, clip, numpy as np, cv2, tqdm\n",
    "\n",
    "def sample_frames(video_path, num_frames=4):\n",
    "    cap, frames = cv2.VideoCapture(video_path), []\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    idxs = np.linspace(0, length - 1, num_frames, dtype=int)\n",
    "    for i in idxs:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ok, frame = cap.read()\n",
    "        if ok: frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames                  # list[np.ndarray]\n",
    "\n",
    "def main(args):\n",
    "    device = \"mps\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    all_feats, names = [], []\n",
    "    for vp in tqdm.tqdm(sorted(glob.glob(os.path.join(args.video_dir, \"*.mp4\")))):\n",
    "        imgs = sample_frames(vp, 4)\n",
    "        with torch.no_grad():\n",
    "            batch = torch.stack([preprocess(Image.fromarray(im)) for im in imgs]).to(device)\n",
    "            feats = model.encode_image(batch).float()          # [N,512]\n",
    "        all_feats.append(feats.mean(0).cpu().numpy())          # mean-pool\n",
    "        names.append(os.path.basename(vp))\n",
    "    np.save(args.out, np.stack(all_feats))\n",
    "    json.dump(names, open(args.out.replace(\".npy\", \".json\"), \"w\"))\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--video_dir\", required=True)\n",
    "    p.add_argument(\"--out\", default=\"feats.npy\")\n",
    "    main(p.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FAISS 余弦索引 (InnerProduct + L2-norm)\n",
    "\"\"\"\n",
    "import numpy as np, faiss, json, argparse\n",
    "\n",
    "p = argparse.ArgumentParser()\n",
    "p.add_argument(\"--feats\", default=\"feats.npy\")\n",
    "p.add_argument(\"--names\", default=\"feats.json\")\n",
    "p.add_argument(\"--out\",   default=\"clip.index\")\n",
    "args = p.parse_args()\n",
    "\n",
    "xb = np.load(args.feats).astype(\"float32\")\n",
    "faiss.normalize_L2(xb)\n",
    "index = faiss.IndexFlatIP(xb.shape[1])\n",
    "index.add(xb)\n",
    "faiss.write_index(index, args.out)\n",
    "print(\"Indexed\", index.ntotal, \"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f031e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python src/search_clip.py --query \"a cat on sofa\" --topk 5\n",
    "\"\"\"\n",
    "import argparse, clip, torch, faiss, numpy as np, json\n",
    "\n",
    "p = argparse.ArgumentParser()\n",
    "p.add_argument(\"--index\", default=\"clip.index\")\n",
    "p.add_argument(\"--names\", default=\"feats.json\")\n",
    "p.add_argument(\"--query\", required=True)\n",
    "p.add_argument(\"--topk\",  type=int, default=5)\n",
    "args = p.parse_args()\n",
    "\n",
    "device = \"mps\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "with torch.no_grad():\n",
    "    q = model.encode_text(clip.tokenize([args.query]).to(device)).float()\n",
    "q = q / q.norm(dim=-1, keepdim=True)\n",
    "index = faiss.read_index(args.index)\n",
    "D, I = index.search(q.cpu().numpy(), args.topk)\n",
    "names = json.load(open(args.names))\n",
    "for score, idx in zip(D[0], I[0]):\n",
    "    print(f\"{score:.3f}\\t{names[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python src/extract_subs.py --video_dir data/samples/video\n",
    "→ 在 data/samples/subtitles/ 生成 *.srt\n",
    "\"\"\"\n",
    "import os, subprocess, argparse, pathlib, tqdm\n",
    "\n",
    "def dump_sub(video_path, out_dir):\n",
    "    vid = pathlib.Path(video_path).stem\n",
    "    srt_out = f\"{out_dir}/{vid}.srt\"\n",
    "    if os.path.exists(srt_out):         # 跳过已提取\n",
    "        return\n",
    "    # ① 先试 ffmpeg 内嵌字幕流\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-map\", \"0:s:0\",\n",
    "           srt_out, \"-loglevel\", \"quiet\"]\n",
    "    if subprocess.call(cmd) != 0:\n",
    "        # ② 若失败，生成空 srt，占位\n",
    "        open(srt_out, \"w\").close()\n",
    "\n",
    "def main(args):\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    for mp4 in tqdm.tqdm(sorted(pathlib.Path(args.video_dir).glob(\"*.mp4\"))):\n",
    "        dump_sub(str(mp4), args.out_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--video_dir\", required=True)\n",
    "    p.add_argument(\"--out_dir\", default=\"data/samples/subtitles\")\n",
    "    main(p.parse_args())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
